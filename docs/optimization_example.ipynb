{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodiff.structures import Number\n",
    "from autodiff.structures import Array\n",
    "from autodiff.optimizations import bfgs_symbolic\n",
    "from autodiff.optimizations import bfgs\n",
    "from autodiff.optimizations import steepest_descent\n",
    "import timeit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented three optimization methods in our optimization module, steepest descent, AD BFGS and symbolic BFGS. The former two use automatic differentiation to obtain gradient information and the last one requires user input of the expression of the gradient.\n",
    "\n",
    "In this example, we use the classic rosenbrock function to benchmark each optimization method. We time each method to compare efficiency.\n",
    "\n",
    "Let's first take a look at the AD bfgs method: Use Array([Number(2),Number(1)]) as initial guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xstar: [Number(val=1.0000000000025382) Number(val=1.0000000000050797)]\n",
      "Minimum: Number(val=6.4435273497518935e-24)\n",
      "Jacobian at each step: [array([2402, -600]), array([-5.52902304e+12, -1.15187980e+09]), array([-474645.77945484,  127109.93018289]), array([ 1.62663315e+09, -2.70845802e+07]), array([-8619.39185109,  2161.73842208]), array([-144.79886656,   36.76686628]), array([1.99114433e+00, 3.55840767e-04]), array([1.99094760e+00, 4.05114252e-04]), array([1.96305014, 0.00739234]), array([1.9349555 , 0.01442883]), array([1.87896168, 0.02845251]), array([1.79486979, 0.04951253]), array([1.65477644, 0.08459553]), array([1.43057943, 0.14073564]), array([1.06628965, 0.23194665]), array([0.47793023, 0.37924961]), array([-0.47390953,  0.61758141]), array([-2.01036637,  1.00259974]), array([-4.48450951,  1.62438092]), array([-8.45367483,  2.63103416]), array([-14.76319038,   4.28099834]), array([-4.62373441,  1.75866303]), array([141.6035648 , -30.46772268]), array([ 7.1036742 , -1.70213995]), array([10.42215839, -2.72209527]), array([13.5437883 , -3.72650641]), array([16.21168154, -4.6779326 ]), array([16.2320604 , -4.88405766]), array([10.21216085, -3.12699453]), array([ 5.13098972, -1.52792632]), array([14.78025278, -5.5124494 ]), array([-1.52082729,  0.8129434 ]), array([ 3.23558391, -1.1679951 ]), array([ 7.90182702, -3.37442389]), array([ 2.23734678, -0.8351361 ]), array([ 0.98042779, -0.31955033]), array([ 2.87258631, -1.30884655]), array([ 0.71918344, -0.28586232]), array([ 0.36444279, -0.14819236]), array([ 0.46808941, -0.22547022]), array([-0.01787606,  0.01191596]), array([ 0.01227375, -0.00612718]), array([ 0.00044299, -0.00020912]), array([ 6.03745724e-08, -1.87348359e-08]), array([3.74411613e-12, 6.66133815e-13])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time for symbolic bfgs to perform optimization 0.0005920969999999581 total time taken is 0.0005920969999999581\n"
     ]
    }
   ],
   "source": [
    "\n",
    "initial_guess = Array([Number(2),Number(1)])\n",
    "\n",
    "def rosenbrock(x0):\n",
    "    return (1-x0[0])**2+100*(x0[1]-x0[0]**2)**2\n",
    "\n",
    "initial_time2 = timeit.timeit()\n",
    "\n",
    "results = bfgs(rosenbrock,initial_guess)\n",
    "print(\"Xstar:\",results[0])\n",
    "print(\"Minimum:\",results[1])\n",
    "print(\"Jacobian at each step:\",results[2])\n",
    "final_time2 = timeit.timeit()\n",
    "\n",
    "time_for_optimization = initial_time2-final_time2\n",
    "print('\\n\\n\\n')\n",
    "print(\"Time for symbolic bfgs to perform optimization\",time_for_optimization,'total time taken is',time_for_optimization)\n",
    "      \n",
    "      \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total time needed for the entire optimization process is around 0.0003 s.\n",
    "\n",
    "Then, let's compare with the traditional symbolic optimization using bfgs\n",
    "\n",
    "First, the user needs to calculate the derivative either by hand or through sympy. Here we use sympy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function to be derivatized : (1 - x)**2 + 100*(-x**2 + y)**2\n",
      "After Differentiation : -400*x*(-x**2 + y) + 2*x - 2\n",
      "After Differentiation : -200*x**2 + 200*y\n",
      "Time for sympy to find derivative expression 0.0014529649999914795\n"
     ]
    }
   ],
   "source": [
    "from sympy import * \n",
    "import sympy\n",
    "\n",
    "initial_time = timeit.timeit()\n",
    "\n",
    "x, y = symbols('x y') \n",
    "\n",
    "rb = (1-x)**2+100*(y-x**2)**2\n",
    "print(\"Function to be derivatized : {}\".format(rb)) \n",
    "\n",
    "# Use sympy.diff() method \n",
    "par1 = diff(rb, x) \n",
    "par2 = diff(rb,y)\n",
    "print(\"After Differentiation : {}\".format(par1)) \n",
    "print(\"After Differentiation : {}\".format(par2)) \n",
    "\n",
    "def gradientRosenbrock(x0):\n",
    "    x=x0[0]\n",
    "    y=x0[1]\n",
    "    drdx = -2*(1 - x) - 400*x*(-x**2 + y)\n",
    "    drdy = 200 *(-x**2 + y)\n",
    "    return drdx,drdy\n",
    "\n",
    "final_time=timeit.timeit()\n",
    "time_for_sympy = initial_time-final_time\n",
    "print(\"Time for sympy to find derivative expression\",time_for_sympy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time taken for sympy to find the derivative is around 0.0003 second and this can be a lot more if the user calculates derivatives by hand.\n",
    "\n",
    "Second, use symbolic bfgs to perform optimization. Use [2,1] as the initial guess.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xstar: [1. 1.]\n",
      "Minimum: 6.4435273497518935e-24\n",
      "Jacobian at each step: [array([2402, -600]), array([-5.52902304e+12, -1.15187980e+09]), array([-474645.77945484,  127109.93018289]), array([ 1.62663315e+09, -2.70845802e+07]), array([-8619.39185109,  2161.73842208]), array([-144.79886656,   36.76686628]), array([1.99114433e+00, 3.55840767e-04]), array([1.99094760e+00, 4.05114252e-04]), array([1.96305014, 0.00739234]), array([1.9349555 , 0.01442883]), array([1.87896168, 0.02845251]), array([1.79486979, 0.04951253]), array([1.65477644, 0.08459553]), array([1.43057943, 0.14073564]), array([1.06628965, 0.23194665]), array([0.47793023, 0.37924961]), array([-0.47390953,  0.61758141]), array([-2.01036637,  1.00259974]), array([-4.48450951,  1.62438092]), array([-8.45367483,  2.63103416]), array([-14.76319038,   4.28099834]), array([-4.62373441,  1.75866303]), array([141.6035648 , -30.46772268]), array([ 7.1036742 , -1.70213995]), array([10.42215839, -2.72209527]), array([13.5437883 , -3.72650641]), array([16.21168154, -4.6779326 ]), array([16.2320604 , -4.88405766]), array([10.21216085, -3.12699453]), array([ 5.13098972, -1.52792632]), array([14.78025278, -5.51244941]), array([-1.52082729,  0.8129434 ]), array([ 3.23558391, -1.16799509]), array([ 7.90182703, -3.3744239 ]), array([ 2.23734678, -0.8351361 ]), array([ 0.9804278 , -0.31955033]), array([ 2.87258632, -1.30884655]), array([ 0.71918344, -0.28586232]), array([ 0.36444279, -0.14819236]), array([ 0.46808941, -0.22547022]), array([-0.01787606,  0.01191596]), array([ 0.01227375, -0.00612718]), array([ 0.00044299, -0.00020912]), array([ 6.03746022e-08, -1.87348803e-08]), array([3.74411613e-12, 6.66133815e-13])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time for symbolic bfgs to perform optimization 0.005963177000012365 total time taken is 0.007416142000003845\n"
     ]
    }
   ],
   "source": [
    "initial_time1 = timeit.timeit()\n",
    "\n",
    "results = bfgs_symbolic(rosenbrock,gradientRosenbrock,[2,1])\n",
    "print(\"Xstar:\",results[0])\n",
    "print(\"Minimum:\",results[1])\n",
    "print(\"Jacobian at each step:\",results[2])\n",
    "final_time1 = timeit.timeit()\n",
    "\n",
    "time_for_optimization_symbolic = initial_time1-final_time1\n",
    "print('\\n\\n\\n')\n",
    "print(\"Time for symbolic bfgs to perform optimization\",time_for_optimization_symbolic,'total time taken is',time_for_optimization_symbolic+time_for_sympy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total time taken for symbolic optimization is 0.007416142000003845 s, while the total time taken for a0.0005920969999999581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
